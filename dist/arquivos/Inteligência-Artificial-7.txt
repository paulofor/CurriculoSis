[Oportunidade 1]
Oportunidade: Machine Learning Engineer - IBP Build

Sobre a vaga
Overview

Are you passionate about shaping the future of IBP impacting all business functions at a global scale?

Join our team to support global digital developments for PepsiCo, focusing on high-quality demand forecasts and new product forecasting within the Integrated Business Planning program. As part of an interdisciplinary team, you'll build and deploy statistical/machine learning models, collaborating with process owners, product owners, and business users, ensuring the criticality of your developments is understood. You will also serve as an internal ambassador, providing expertise and stewardship in data and analytics.

Responsibilities

Your day to day with us

Partner with data engineers to ensure data access for discovery and proper data is prepared for model consumption.
Partner with ML engineers working on industrialization.
Coordinate work activities with Business teams, other IT services and as required.
Drive the use of the Platform toolset and to also focus on 'the art of the possible' demonstrations to the business as needed.
Communicate with business stakeholders in the process of service design, training and knowledge transfer.
Support large-scale experimentation and build data-driven models.
Set KPIs and metrics to evaluate analytics solution given a particular use case.
Refine requirements into modelling problems.
Influence product teams through data-based recommendations.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create reusable packages or libraries.

Qualifications

What you will need to succeed

2+ years’ experience building solutions in the revenue management or in the supply chain space.
2+ years working in a team to deliver production level analytic solutions. Fluent in git (version control). Understanding of Jenkins, Docker are a plus.
3+ years’ experience in ETL and/or data wrangling techniques. Fluent in SQL syntaxis.
3+ years’ experience in Statistical/ML techniques to solve supervised (regression, classification) and unsupervised problems. Experiences with Deep Learning are a plus.
3+ years’ experience in developing business problem related statistical/ML modeling with industry tools with primary focus on Python or R development.
Business storytelling and communicating data insights in business consumable format. Fluent in one Visualization tool.
Strong communications and organizational skills with the ability to deal with ambiguity while juggling multiple priorities
Experience with Agile methodology for team work and analytics ‘product’ creation.

Additional qualifications that will make you stand out for this position

Experience with Azure cloud services is a plus.
Experience with distributed machine learning is a plus.

What makes us different? 

Hybrid work model: combination of remote and collaborative office experience to enable innovation 
Entrepreneurial environment in leading international company 
Professional growth possibilities & learning opportunities 
Variety of benefits to support your physical, emotional and financial wellbeing 
Volunteering opportunities to help external communities 
Diverse team with more than 30% of female representation & over 30 nationalities 
Have a stake in D&I strategy and bring your whole self to work 

About PepsiCo

We believe that culture should be at the cornerstone of everything we do at PepsiCo. We are agile, innovative and not afraid of failure. We want our team to come to work every day excited to explore new ways to bring enjoyment, refreshment and fun to the world.

PepsiCo Positive (pep+) is the future of our organization – a strategic end-to-end transformation, with sustainability at the center of how we will create growth and value by operating within planetary boundaries and inspiring positive change for the planet and people.

So, if you’re ready to be a part of a playground for those who think big, we’d love to chat.

*We encourage the diversity of applicants across gender, age, ethnicity, nationality, sexual orientation, social background, religion or belief and disability 

#Locations: Barcelona, Spain; Vitoria-Gasteiz, Spain
-----------------------------------------------------------------------------------
[Oportunidade 2]
Oportunidade: Grupo QuintoAndar | Senior Data Platform Engineer

Sobre a vaga
About QuintoAndar

QuintoAndar was born to do something very rewarding: open doors. We opened doors for technology to be part of living. And, through it, we simplify and reduce the bureaucracy of the experience of those looking for a new home. Thus, we became the most valuable proptech in Latin America, leading the real estate market in around six countries and more than 75 cities around the globe.

The first steps in the international market have been taken since we acquired Grupo Navent, which is ranked number 1 in Mexico (Inmuebles24), Argentina (Zonaprop), Peru (Adondevivir & Urbania) and Ecuador (Plusvalia) - among others. And most recently, we launched Benvi, our international brand in Mexico, which comes with our residential rental product and service.

We offer an end-to-end ecosystem to make life easier for those looking to rent or buy a home, sell a property and unlock a transaction. Think of zero paper or bureaucracy: everything happens on a single platform, with transparency and speed alongside incredible human support.

We are currently valued at over $5.1 billion (August 2021) and continue to grow 4x year over year, breaking records.

To make all this happen, we have more than 4,000 talented people, working with cutting-edge technology and best design practices to ensure a seamless experience throughout the sales and rental process, combined with smart financial products. Here you will work with the best professionals in the market, in an environment that breathes innovation, collaboration and high performance.

To learn more about our history, visit https://quintoandar.group/en/.

About Grupo QuintoAndar:

We are Grupo QuintoAndar, the largest real estate ecosystem in Latin America. Guided by a shared purpose of helping people love where they live, we have a diverse portfolio of brands and solutions that cover all stages of the journey of living. We develop technologies and innovations that transform and leverage the whole living experience.

About Working At QuintoAndar

Possibility of learning;
Opportunity to work in a team that seeks to use the best practices and tools in the market;
Work in an informal environment with a horizontal structure;
Being part of a team working on a high-impact project that affects the lives of thousands of people

Location & Remote Work

Our technology team works on the "remote-first" model, which means we are working from home with the possibility of living anywhere in Brazil. We also have the option of using QuintoAndar's offices in São Paulo and Campinas or using partner coworking spaces, both up to twice a week.

Language

This job description is written in English because for that position you will need it for communications with coworkers and suppliers who are from worldwide, for tools and internal materials as well. // A descrição desta vaga está escrita em Inglês porque para esta posição é necessário para a comunicação com colegas e fornecedores, que são de diferentes países, além do uso de ferramentas e materiais internos.

Stages of the Selection Process

Our selection process currently lasts an average of 30-40 days, from application to completion of the assessment. Going through:

Application 
Tech Screening 
Technicals Interviews with DPE Team
Interview with Recruiter
Offer

About The Position

The main goal of the data engineering team is to build and maintain data pipelines and processes to create a high-performance, self-service platform for Analytics Engineers, Data Analysts, and Data Scientists. This platform will enable them to build excellent pipelines easily, with abstractions ready for reliability, scalability, performance, security, quality, profiling, documentation, lineage, and more.

Responsibilities of a Data Platform Engineer at QuintoAndar:

Build and maintain a high-performance data platform that meets the company's needs, connects with product solutions, and leads analytical innovation, enabling incredible architectures and efficient platforms;
Responsible for the entire code development lifecycle (monitoring deployment, adding metrics and alarms, ensuring SLO budget compliance, and more);
Align with stakeholders to understand their primary needs, while also having a holistic view of the problem and proposing extensible, scalable, and incremental solutions;
Conduct PoCs and benchmarks to determine the best tool for a given problem, and decide whether to use an off-the-shelf solution or develop one in-house;
Contribute to defining the strategic vision, crossing team and service boundaries to solve problems

Here are some amazing projects we've worked on:

We moved our entire legacy structure with transactional strategies to distributed processing environments, gaining more scale and flexibility;
We designed an architecture capable of storing metadata and exposing a data catalog to the entire company, thereby increasing reliability and understanding;
We also worked on LGPD compliance by connecting anonymization strategies, access control, and metadata management to ensure secure access and sharing;
We centralized access to analytical data to separate data storage from its processing, retention, and discovery

Requirements

What we are looking for:

Knowledge in Big Data technologies, solutions, and concepts (Spark, Hive, MapReduce) and multiple languages (YAML, Python);
Proficiency in Python, or one of the main programming languages, and a passion for writing clean and maintainable code. You may even be a Software Engineer with a focus or passion for data-oriented solutions;
Understanding of the data lifecycle and concepts such as lineage, governance, privacy, retention, anonymization, etc.;
Excellent communication skills, proactively sharing and seeking context to collaborate with different teams

You will stand out if:

You have participated in building large-scale data platforms for big data sets and teams using Big Data technologies such as Spark, Trino, Hive, Atlas, Ranger, etc.;
You have knowledge of AWS and GCP cloud resources;
You have experience with Apache Airflow or other tools for data workflows;
You have experience with columnar storage solutions and/or data lakehouse concepts;
You have experience with Databricks Platform resources;
You have knowledge in infrastructure areas such as containers and orchestration (Kubernetes, ECS), CI/CD strategies, infrastructure as code (Terraform), observability (Prometheus, Grafana), among others;
You understand how optimization works in various scenarios and can be improved, such as query performance, service scalability, and cluster management;
You are curious about rapidly evolving technologies in the data engineering domain and eager to learn and master them, bringing a significant impact to our team

Important:

Our selection process starts with the application! If you are truly interested in joining our team, make sure to put in extra effort at this stage. We review all candidates individually and provide feedback even to those who do not proceed in the process;
All communication is done via email, so be attentive to our messages and whitelist the @quintoandar.com.br domain to prevent our emails from going to spam

Benefits

Competitive salary package;
Bonus
Meal allowance ("Flash benefícios");
Health plan;
Dental plan (optional);
Life insurance;
Daycare subsidy;
Subsidy to sports practicing (Gympass)
Extended maternity and paternity leave;
Reserved room for breast-feeding*;
Discount on our parking lot;*
Language learning support;
Free transfer from Vila Madalena and Fradique Coutinho stations to the office*;
Free bike rack in our parking lot.*

Diversity & Inclusion at QuintoAndar

At QuintoAndar, we believe diversity of perspectives and experiences guarantee a differentiated work environment, based on respect and valuing differences. Feel free to declare the information on the registration form. If you are not comfortable answering them, just choose the option "I prefer not to respond". This information helps us create an increasingly inclusive environment and it is used only for this purpose, it is confidential, and it will not impact your performance throughout the hiring process.

Privacy and Data Protection

In order to apply for one of our jobs roles, we will need to collect some of your personal data necessary for us to review your application and to contact you. We believe that the diversity of perspectives and experiences guarantees a differentiated work environment, based on respect and appreciation of differences. For this reason, we have several affirmative jobs, and information regarding your gender, ethnicity/race, and disability may be collected in the process.

All data processed is confidential and will be stored in a secure place for the time necessary to fulfill its purposes, with appropriate technical and administrative measures being adopted to protect your information.

If you have any questions, please contact us via the following page.
-----------------------------------------------------------------------------------
[Oportunidade 3]
Oportunidade: Senior Data Engineer

Sobre a vaga
Location: Rua Mariano Torres 729, Floor 9º.; Curitiba - PR, 80060-120
CLT Position (Consolidação das Leis do Trabalho)
Hybrid Work Model: 2 days office, 3 days remote.

Our mission is to unlock human potential. We welcome you for who you are, and the background you bring, and we embrace individuals who get excited about learning. Bring your experiences, your perspectives, and your passion; it’s in our differences that we empower the way the world learns.

About the Role:
This position within the Wiley Data Analytics & Insights team, is responsible for functioning as the Senior Data Engineer, to develop and maintain data pipelines from multiple data sources into our Data Lake environment.
This individual will work very closely with the other development leads, data engineers, and data architects to build and maintain data pipelines going to different zones of the Snowflake Data Warehouse environment. Determine the optimal approach for extracting, transforming, and loading data into different zones of Data Lake (Raw/Native, Processed/Transformed, Enriched, Archive). This includes design and development to prepare data for movement storage & consumption Views, ELT Processes, Extracts & other processes that manipulate, aggregate, clean, or enrich the data. The individual should demonstrate a thorough understanding of business validations and data quality. They should apply these principles diligently within our processes, aligning with the specific use cases.
This data lake would see regular addition of datasets from internal and external sources deemed important for Wiley’s data analytics and insights requirements. In some cases, this role may involve building repeatable data feeds to and from platforms such as, Salesforce, SAP, MDM, Eloqua, e-commerce websites, etc. The position requires development skills to enable the matching and linking of internal and external data sources (especially as it relates to Customer and products) using analytical techniques/tools. The role will have frequent interaction/collaboration with Solutions Architects, Data Architects, BI & Analytics Development, Data Engineering, Marketing, Sales, Application/Product teams and Enterprise Architects to support Wiley’s Data Analytics and Insights roadmap.

How you will make an impact:
Design and implement secure data pipelines into a Snowflake data warehouse from on-premise and cloud data sources
Define best practices and standards for data pipelining and integration with Snowflake data lake and warehouses in collaboration with Data Architect and other Data leads
Design and implement high-performing data pipelines feeding downstream systems
Collaborate with onshore/ offshore data engineers, BA, and Scrum Masters who will be building or testing data applications in Snowflake environment.
Design and implement high-performing BI dashboard integrations Cognos and QlikView working with reporting and data visualization developers
Ensure enterprise security and access control policies are adhered to in the solution
Creation of architecture and design artifacts and documents
Conduct design and code reviews

What we look for:
B.S Computer Science or related degree
4+ years experience in data pipeline architecture and data modelling
Demonstrably deep understanding of SQL and relational databases (Snowflake preferred)
Experience with Snowflake SnowSQL and writing user-defined functions would be a plus
Hands-on experience with Python, Scala, Java, SQL, and tools like NIFI, Attunity, Spark (Databricks/Qubole), etc.
Hands-on experience with data pipeline tools within cloud-based environments (Argo, Airflow, Luigi, DBT)
Hands-on experience with CI/CD tools and practices (CircleCI, Travis, Jenkins)
Hands-on experience with AWS (SQS, S3, Kinesys, Firehose, MKS, Lambda)
Experience with open-source data warehouse tools
Constantly improve product quality, security, and performance
Write good code, performant code (Python preferred)

About Wiley:
Wiley is a trusted leader in research and learning, our pioneering solutions and services are paving the way for knowledge seekers as they work to solve the world’s most important challenges. We are advocates of advancement, empowering knowledge-seekers to transform today’s biggest obstacles into tomorrow’s brightest opportunities.
With over 200 years of experience in publishing, we continue to evolve knowledge seekers’ steps into strides, illuminating their path forward to personal, educational, and professional success at every stage. Around the globe, we break down barriers for innovators, empowering them to advance discoveries in their fields, adapt their workforces, and shape minds.
When applying, please attach your resume/CV to be considered.
-----------------------------------------------------------------------------------
[Oportunidade 4]
Oportunidade: Senior Data Engineer (TV Platform)

Sobre a vaga
Sigma Software is looking for an experienced Senior Data Engineer to join our growing engineering team. This opportunity is for you if you want to work with a tightly knit team of data engineers solving challenging problems using cutting-edge data collection, transformation, analysis, and monitoring tools in the cloud.

Sounds like you?
We are waiting for you on our team!

CUSTOMER
Our client has superior end-to-end technology, a premium marketplace, and best-in-market advisory services that power the world’s advertising businesses of the largest media and entertainment companies. For example, Fox, NBC Universal, Viacom in the USA, Sky, Channel 4, RTE, and Mediaset in Europe.

PROJECT
We invite you to work with the provider of comprehensive ad platforms for publishers, advertisers, and media buyers. We build and support high-quality data solutions to process terabytes of data on AWS’s cloud-native data platform.

RESPONSIBILITIES
Build and maintain ETL pipelines
Modify existing application code or interfaces or build new application components
Analyze requirements, support design, code, test, debug, deploy, and maintain programs and interfaces. Documentation of the work is essential
Develop and implement databases, data collection systems, data analytics, and other strategies that optimize statistical efficiency and quality
Conduct code and design reviews to ensure the high quality of the product
Mentor and guide colleagues and new team members
Collaborate with Data engineers and Product Managers to prioritize business needs and translate complex product requirements into working high-quality cloud-native data solutions
Share knowledge with wider engineering teams by doing technical demos

REQUIREMENTS
5+ years of hands-on experience in the Software Development field and/or Big Data
Excellent knowledge of Python
Proficient in PySpark and a strong understanding of Spark
Understanding of ETL frameworks such as DBT
Experience with orchestration tools such as Airflow
Knowledge of Lakehouse architecture on top of AWS such as Apache Iceberg, Hudi, or Delta Lake
Strong understanding of AWS (IAM, S3, Security groups)
Proficient in Infra as code (Terraform or similar)
Great communication skills: Able to articulate clearly about status, blockers, and design
Ability to work independently and collaboratively
At least an Upper-Intermediate level of English

WOULD BE A PLUS
Experience dealing with modern aspects of Lakehouse such as Databricks Unity Catalog and Snowflake Iceberg integration
-----------------------------------------------------------------------------------
[Oportunidade 5]
Oportunidade: Cientista de Dados

Sobre a vaga
A LTrace está em busca de candidato com experiência em Deep Learning e Python para fazer parte de um time de pesquisa e desenvolvimento de soluções para a indústria de petróleo e gás. 
Entre as atividades a serem realizadas está a pesquisa e desenvolvimento de modelos generativos como GANs e VAEs (2D e 3D) em PyTorch/Tensorflow aplicado na área de atualização de modelos de reservatório. 
O objetivo é aplicar deep learning para otimizar o processo de simulação de reservatórios para ajuste de histórico com assimilação de dados para tomadas de decisões na perfuração de poços de exploração e produção.

Local de Trabalho: 
Remoto CLT
Formação necessária: 
Ensino superior completo em exatas.
Conhecimentos obrigatórios:
Python
PyTorch/Tensorflow
Deep Learning
Generative Adversarial Nets
Autoencoders
Diferenciais:
Mestrado/Doutorado na área de Inteligência Artificial
Inglês
Benefícios:
Vale-alimentação
Vale-refeição
Horário de trabalho:
Turno de 8 horas

O processo seletivo consiste em duas etapas:
Envio de um teste para o desenvolvimento de um modelo VAE e ou GAN básico para a geração de imagens. Fornecemos um pequeno conjunto de dados e algumas orientações.
Entrevista com discussão da solução e avaliação por parte do nosso time.

Enviar CV para contact@ltrace.com.br ou candidatar-se pelo linkedin
-----------------------------------------------------------------------------------
