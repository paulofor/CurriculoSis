[Oportunidade 1]
Oportunidade: Engenharia de Dados AWS - S√™nior

Sobre a vaga
Sobre a vaga

Com mais de 35 anos de mercado e localizada em 16 pa√≠ses, a GFT emprega mais de 10.000 pessoas ao redor do mundo sendo 3.000 pessoas no Brasil.
Aqui, a diversidade, a igualdade e a inclus√£o est√£o no centro e garantir um ambiente de trabalho seguro para todas as pessoas faz parte de quem somos.
Promovemos uma cultura de desenvolvimento e crescimento pautadas no nosso pilar de Continuous Scaled Learning porque acreditamos o uso inteligente da tecnologia √© a chave para o sucesso nesse mundo digital.

O que estamos buscando?

Requisitos:
Experi√™ncia com integra√ß√£o e processamento de dados
Experi√™ncia na constru√ß√£o de processos ETL e fluxos de dados
S√≥lidos conhecimentos com SQL
Conhecimento com modelagem de dados multidimensional
Conhecimento em Cloud Computing (AWS)
Experi√™nncia com Athena, Glue e QuickSight

Atua√ß√£o 100% remota

Descri√ß√£o comportamental: 
Procuramos uma pessoa que:
Goste de trabalhar em equipe e seja colaborativa em suas atribui√ß√µes;
Tenha coragem para se desafiar e ir al√©m, abra√ßando novas oportunidades de crescimento;
Transforme ideias em solu√ß√µes criativas e busque qualidade em toda sua rotina;
Tenha habilidades de resolu√ß√£o de problemas;
Possua habilidade e se sinta confort√°vel para trabalhar de forma independente e gerenciar o pr√≥prio tempo;
Tenha interesse em lidar com situa√ß√µes adversas e inovadoras no √¢mbito tecnol√≥gico.

O que oferecemos:

Cart√£o multi-benef√≠cios ‚Äì voc√™ escolhe como e onde utilizar;
Bolsas de Estudos para cursos de Gradua√ß√£o, P√≥s, MBA e Idiomas;
Programas de incentivo √† Certifica√ß√µes;
Hor√°rio de trabalho flex√≠vel;
Plano personalizado de carreira e possibilidade de carreira internacional;
Sal√°rios competitivos;
Avalia√ß√£o de desempenho anual com possibilidade de bonifica√ß√£o;
Gympass;
Previd√™ncia Privada;
Aux√≠lio-Creche;
Assist√™ncia M√©dica;
Assist√™ncia Odontol√≥gica;
Seguro de Vida.

Big enough to deliver ‚Äì small enough to care.
#VempraGFT
#VamosVoarJuntos
#ProudToBeGFT
-----------------------------------------------------------------------------------
[Oportunidade 2]
Oportunidade: Senior Machine Learning Engineer

Sobre a vaga
Vaga: Machine Learning Engineer 

Sobre a ATRA Informatica:
A ATRA Informatica √© uma consultoria de TI especializada em Dados, l√≠der no mercado e localizada em S√£o Paulo. Oferecemos uma cultura de inova√ß√£o e crescimento cont√≠nuo para nossos colaboradores.

Descri√ß√£o da Vaga:
Estamos em busca de um(a) Machine Learning Engineer especializado(a) em Google Cloud Platform (GCP) para integrar nosso time. O profissional ser√° respons√°vel pelo desenvolvimento, implanta√ß√£o e manuten√ß√£o de modelos de Machine Learning, utilizando tecnologias de ponta para resolver problemas complexos e melhorando produtos e servi√ßos.

Responsabilidades:
Desenvolver e implementar modelos de Machine Learning utilizando GCP 
Utilizar bibliotecas de Python, como Scikit-learn, TensorFlow e Pandas, para o desenvolvimento de solu√ß√µes 
Trabalhar com Intelig√™ncia Artificial Generativa para criar solu√ß√µes inovadoras 
Utilizar ferramentas como Gemini e Vertex AI para gerenciamento e deploy de modelos 
Desenvolver e gerenciar chatbots utilizando Dialogflow CX 
Colaborar com equipes multidisciplinares para entender as necessidades do neg√≥cio e traduzir requisitos em solu√ß√µes t√©cnicas
Monitorar e otimizar a performance dos modelos em produ√ß√£o 
Documentar processos e solu√ß√µes desenvolvidas 

Requisitos:
Experi√™ncia comprovada como Machine Learning Engineer 
Profici√™ncia em Python e bibliotecas como Scikit-learn, TensorFlow e Pandas 
Experi√™ncia com Google Cloud Platform (GCP) 
Conhecimento em Intelig√™ncia Artificial Generativa 
Experi√™ncia com Gemini e Vertex AI 
Experi√™ncia com desenvolvimento e gerenciamento de chatbots utilizando Dialogflow CX 
Habilidade para trabalhar em equipe e boa comunica√ß√£o 
Gradua√ß√£o em Ci√™ncia da Computa√ß√£o, Engenharia, Matem√°tica ou √°reas relacionadas 

Benef√≠cios:
üìö Treinamentos e cursos em tecnologias de dados de mercado 
üí≥ Aux√≠lio com certifica√ß√µes (100% de reembolso) 
üìä Trilhas de conhecimento em tecnologias de dados e cloud 
üéì Parcerias com institui√ß√µes de ensino para p√≥s-gradua√ß√£o 
üèãÔ∏è‚Äç‚ôÇÔ∏è Totalpass 
üß† Parcerias com profissionais de psicologia e nutri√ß√£o para o seu bem-estar 
üí∞ Contrata√ß√£o PJ e sal√°rio negoci√°vel 

üåç Localiza√ß√£o: Remoto

Se voc√™ tem o perfil que buscamos e deseja fazer parte de um ambiente din√¢mico e desafiador, envie seu curr√≠culo! Venha fazer a diferen√ßa na governan√ßa de dados!

Interessado(a)? Candidate-se!

Estamos ansiosos para conhecer voc√™! üåü

#Vagas #MachineLearning #GCP #Intelig√™nciaArtificial #Tecnologia #Dados #Oportunidade #Carreira #TrabalhoRemoto #ATRAInformatica
-----------------------------------------------------------------------------------
[Oportunidade 3]
Oportunidade: Senior Data Engineer

Sobre a vaga
A Keyrus acredita na diversidade e na inclus√£o. Encorajamos a todos a participarem em nosso processo de contrata√ß√£o, n√£o importando o g√™nero, idade, ra√ßa, religi√£o. N√£o permitimos nenhum tipo de discrimina√ß√£o. Isto √© refor√ßado no processo de contrata√ß√£o e vivido dessa forma na empresa.

Sabemos que grandes resultados s√≥ s√£o alcan√ßados com uma grande equipe, por isso procuramos pessoas talentosas e apaixonadas, com desejo de crescer profissionalmente e criar uma trajet√≥ria de carreira conosco.

Pap√©is e Responsabilidades

Desenvolver novos produtos de dados para o neg√≥cio e garantir a sustenta√ß√£o de

pipelines legados atrav√©s do uso de t√©cnicas de transforma√ß√£o (ETL/ELT) e modelagem

de dados;

Processar dados em near real time;

Garantir a qualidade dos dados;

Garantir que os dados sejam armazenados, processados e compartilhados de forma

segura e em conformidade com as leis e regulamentos de prote√ß√£o de dados, como

LGPD.

Lidar com grandes volumes de dados provenientes de diferentes fontes (Big Data)

Possuir boa interface com o neg√≥cio

Perfil proativo

Realizar levantamento de requisitos para aux√≠lio na modelagem de novos produtos de

dados.

Qualifica√ß√µes Essenciais

Docker
Ecosistema Hadoop
Confluent Cloud (Desej√°vel)
Ingest√£o de dados avan√ßado
Ingest√µes BIG Data
Ingest√µes Batch e Streaming
Kafka
DMS
Hudi
Modelagem de dados avan√ßado
AWS intermedi√°rio
S3
Redshift
EMR
Athena
Glue
DynamoDB

Qualifica√ß√µes Desej√°veis

Conhecimento no Mercado Segurador, Financeiro, Imobili√°rio ou Cr√©dito sera

um diferencial.

Nossa miss√£o √© Auxiliar as empresas a extrair todo o potencial de Dados e Digital objetivando aumentar seu desempenho, ajudando na transforma√ß√£o, gerando novas alavancas de crescimento e competitividade.

Vem fazer parte do Time Keyrus!

Se voc√™ quiser saber mais sobre n√≥s, convidamos voc√™ a visitar nosso site: http://www.keyrus.com/br/

Desejamos boa sorte! üòäüöÄ
-----------------------------------------------------------------------------------
[Oportunidade 4]
Oportunidade: Senior Data Engineer

Sobre a vaga
Location: Rua Mariano Torres 729, Floor 9¬∫.; Curitiba - PR, 80060-120
CLT Position (Consolida√ß√£o das Leis do Trabalho)
Hybrid Work Model: 2 days office, 3 days remote.

Our mission is to unlock human potential. We welcome you for who you are, and the background you bring, and we embrace individuals who get excited about learning. Bring your experiences, your perspectives, and your passion; it‚Äôs in our differences that we empower the way the world learns.

About the Role:
This position within the Wiley Data Analytics & Insights team, is responsible for functioning as the Senior Data Engineer, to develop and maintain data pipelines from multiple data sources into our Data Lake environment.
This individual will work very closely with the other development leads, data engineers, and data architects to build and maintain data pipelines going to different zones of the Snowflake Data Warehouse environment. Determine the optimal approach for extracting, transforming, and loading data into different zones of Data Lake (Raw/Native, Processed/Transformed, Enriched, Archive). This includes design and development to prepare data for movement storage & consumption Views, ELT Processes, Extracts & other processes that manipulate, aggregate, clean, or enrich the data. The individual should demonstrate a thorough understanding of business validations and data quality. They should apply these principles diligently within our processes, aligning with the specific use cases.
This data lake would see regular addition of datasets from internal and external sources deemed important for Wiley‚Äôs data analytics and insights requirements. In some cases, this role may involve building repeatable data feeds to and from platforms such as, Salesforce, SAP, MDM, Eloqua, e-commerce websites, etc. The position requires development skills to enable the matching and linking of internal and external data sources (especially as it relates to Customer and products) using analytical techniques/tools. The role will have frequent interaction/collaboration with Solutions Architects, Data Architects, BI & Analytics Development, Data Engineering, Marketing, Sales, Application/Product teams and Enterprise Architects to support Wiley‚Äôs Data Analytics and Insights roadmap.

How you will make an impact:
Design and implement secure data pipelines into a Snowflake data warehouse from on-premise and cloud data sources
Define best practices and standards for data pipelining and integration with Snowflake data lake and warehouses in collaboration with Data Architect and other Data leads
Design and implement high-performing data pipelines feeding downstream systems
Collaborate with onshore/ offshore data engineers, BA, and Scrum Masters who will be building or testing data applications in Snowflake environment.
Design and implement high-performing BI dashboard integrations Cognos and QlikView working with reporting and data visualization developers
Ensure enterprise security and access control policies are adhered to in the solution
Creation of architecture and design artifacts and documents
Conduct design and code reviews

What we look for:
B.S Computer Science or related degree
4+ years experience in data pipeline architecture and data modelling
Demonstrably deep understanding of SQL and relational databases (Snowflake preferred)
Experience with Snowflake SnowSQL and writing user-defined functions would be a plus
Hands-on experience with Python, Scala, Java, SQL, and tools like NIFI, Attunity, Spark (Databricks/Qubole), etc.
Hands-on experience with data pipeline tools within cloud-based environments (Argo, Airflow, Luigi, DBT)
Hands-on experience with CI/CD tools and practices (CircleCI, Travis, Jenkins)
Hands-on experience with AWS (SQS, S3, Kinesys, Firehose, MKS, Lambda)
Experience with open-source data warehouse tools
Constantly improve product quality, security, and performance
Write good code, performant code (Python preferred)

About Wiley:
Wiley is a trusted leader in research and learning, our pioneering solutions and services are paving the way for knowledge seekers as they work to solve the world‚Äôs most important challenges. We are advocates of advancement, empowering knowledge-seekers to transform today‚Äôs biggest obstacles into tomorrow‚Äôs brightest opportunities.
With over 200 years of experience in publishing, we continue to evolve knowledge seekers‚Äô steps into strides, illuminating their path forward to personal, educational, and professional success at every stage. Around the globe, we break down barriers for innovators, empowering them to advance discoveries in their fields, adapt their workforces, and shape minds.
When applying, please attach your resume/CV to be considered.
-----------------------------------------------------------------------------------
[Oportunidade 5]
Oportunidade: Engenharia de Dados AWS - S√™nior

Sobre a vaga
Sobre a vaga

Com mais de 35 anos de mercado e localizada em 16 pa√≠ses, a GFT emprega mais de 10.000 pessoas ao redor do mundo sendo 3.000 pessoas no Brasil.
Aqui, a diversidade, a igualdade e a inclus√£o est√£o no centro e garantir um ambiente de trabalho seguro para todas as pessoas faz parte de quem somos.
Promovemos uma cultura de desenvolvimento e crescimento pautadas no nosso pilar de Continuous Scaled Learning porque acreditamos o uso inteligente da tecnologia √© a chave para o sucesso nesse mundo digital.

O que estamos buscando?

Requisitos:
Experi√™ncia com integra√ß√£o e processamento de dados
Experi√™ncia na constru√ß√£o de processos ETL e fluxos de dados
S√≥lidos conhecimentos com SQL
Conhecimento com modelagem de dados multidimensional
Conhecimento em Cloud Computing (AWS)
Experi√™nncia com Athena, Glue e QuickSight

Atua√ß√£o 100% remota

Descri√ß√£o comportamental: 
Procuramos uma pessoa que:
Goste de trabalhar em equipe e seja colaborativa em suas atribui√ß√µes;
Tenha coragem para se desafiar e ir al√©m, abra√ßando novas oportunidades de crescimento;
Transforme ideias em solu√ß√µes criativas e busque qualidade em toda sua rotina;
Tenha habilidades de resolu√ß√£o de problemas;
Possua habilidade e se sinta confort√°vel para trabalhar de forma independente e gerenciar o pr√≥prio tempo;
Tenha interesse em lidar com situa√ß√µes adversas e inovadoras no √¢mbito tecnol√≥gico.

O que oferecemos:

Cart√£o multi-benef√≠cios ‚Äì voc√™ escolhe como e onde utilizar;
Bolsas de Estudos para cursos de Gradua√ß√£o, P√≥s, MBA e Idiomas;
Programas de incentivo √† Certifica√ß√µes;
Hor√°rio de trabalho flex√≠vel;
Plano personalizado de carreira e possibilidade de carreira internacional;
Sal√°rios competitivos;
Avalia√ß√£o de desempenho anual com possibilidade de bonifica√ß√£o;
Gympass;
Previd√™ncia Privada;
Aux√≠lio-Creche;
Assist√™ncia M√©dica;
Assist√™ncia Odontol√≥gica;
Seguro de Vida.

Big enough to deliver ‚Äì small enough to care.
#VempraGFT
#VamosVoarJuntos
#ProudToBeGFT
-----------------------------------------------------------------------------------
